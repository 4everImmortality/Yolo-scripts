import os
import shutil
import random
from pathlib import Path

# --- 1. é…ç½®ä½ çš„è·¯å¾„å’Œå‚æ•° ---

# è¾“å…¥ç›®å½•
SOURCE_IMAGES_DIR = Path("VEDAI/images")
SOURCE_LABELS_DIR = Path("VEDAI/labels")

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path("output_datasets")

# æ•°æ®é›†åˆ’åˆ†æ¯”ä¾‹ï¼ˆä¾‹å¦‚ï¼Œ0.2 è¡¨ç¤º 20% çš„æ•°æ®ä½œä¸ºéªŒè¯é›†ï¼‰
VAL_SPLIT_RATIO = 0.2

# âš ï¸ã€é‡è¦ã€‘è¯·åœ¨è¿™é‡Œä¿®æ”¹ä¸ºä½ è‡ªå·±çš„ç±»åˆ«åç§°ï¼é¡ºåºå¿…é¡»å’ŒYOLOæ ‡ç­¾ä¸­çš„ç±»åˆ«ç´¢å¼•ä¸€è‡´ã€‚
CLASS_NAMES = ['person', 'car', 'bicycle'] # è¿™æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œè¯·åŠ¡å¿…ä¿®æ”¹

# --- 2. è„šæœ¬ä¸»é€»è¾‘ (é€šå¸¸æ— éœ€ä¿®æ”¹) ---

def create_yolo_datasets():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºŽåˆ›å»ºå’Œç»„ç»‡ä¸‰ä¸ªYOLOæ•°æ®é›†ã€‚
    """
    print("ðŸš€ å¼€å§‹å¤„ç†æ•°æ®é›†...")

    # æ£€æŸ¥è¾“å…¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not SOURCE_IMAGES_DIR.is_dir() or not SOURCE_LABELS_DIR.is_dir():
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æºæ–‡ä»¶å¤¹ '{SOURCE_IMAGES_DIR}' æˆ– '{SOURCE_LABELS_DIR}'ã€‚")
        print("è¯·ç¡®ä¿è„šæœ¬ä¸Ž 'images' å’Œ 'labels' æ–‡ä»¶å¤¹åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚")
        return

    # ä»Žæ ‡ç­¾æ–‡ä»¶ä¸­èŽ·å–æ‰€æœ‰æ–‡ä»¶åçš„â€œä¸»å¹²â€éƒ¨åˆ† (ä¾‹å¦‚ '00000000')
    base_filenames = sorted([p.stem for p in SOURCE_LABELS_DIR.glob("*.txt")])
    
    if not base_filenames:
        print("âŒ é”™è¯¯ï¼šåœ¨ 'labels' æ–‡ä»¶å¤¹ä¸­æ²¡æœ‰æ‰¾åˆ° .txt æ ‡æ³¨æ–‡ä»¶ã€‚")
        return
        
    print(f"âœ… æ‰¾åˆ°äº† {len(base_filenames)} ä¸ªæ ‡æ³¨æ–‡ä»¶ï¼Œå¯¹åº” {len(base_filenames) * 2} å¼ å›¾åƒã€‚")

    # éšæœºæ‰“ä¹±å¹¶åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    random.shuffle(base_filenames)
    split_index = int(len(base_filenames) * (1 - VAL_SPLIT_RATIO))
    train_files = base_filenames[:split_index]
    val_files = base_filenames[split_index:]

    print(f"ðŸ“Š æ•°æ®é›†åˆ’åˆ†: {len(train_files)} ä¸ªç”¨äºŽè®­ç»ƒ, {len(val_files)} ä¸ªç”¨äºŽéªŒè¯ã€‚")

    # å®šä¹‰è¦åˆ›å»ºçš„æ•°æ®é›†
    datasets_to_create = {
        "dataset_visible": ["co"],
        "dataset_thermal": ["ir"],
        "dataset_combined": ["co", "ir"]
    }

    # ä¸ºæ¯ä¸ªæ•°æ®é›†ç±»åž‹è¿›è¡Œå¤„ç†
    for dataset_name, suffixes in datasets_to_create.items():
        print(f"\nProcessing dataset: {dataset_name}...")
        dataset_path = OUTPUT_DIR / dataset_name
        
        # åˆ›å»º train/val æ–‡ä»¶å¤¹ç»“æž„
        train_images_path = dataset_path / "images" / "train"
        val_images_path = dataset_path / "images" / "val"
        train_labels_path = dataset_path / "labels" / "train"
        val_labels_path = dataset_path / "labels" / "val"
        
        for p in [train_images_path, val_images_path, train_labels_path, val_labels_path]:
            p.mkdir(parents=True, exist_ok=True)
            
        # å¤„ç†æ–‡ä»¶å¹¶å¤åˆ¶
        process_split(train_files, suffixes, train_images_path, train_labels_path, "train")
        process_split(val_files, suffixes, val_images_path, val_labels_path, "val")

        # åˆ›å»º dataset.yaml æ–‡ä»¶
        create_yaml_file(dataset_path, CLASS_NAMES)
        
    print("\nðŸŽ‰ æ‰€æœ‰æ•°æ®é›†å¤„ç†å®Œæˆï¼")
    print(f"è¯·æ£€æŸ¥ '{OUTPUT_DIR}' æ–‡ä»¶å¤¹ã€‚")


def process_split(file_list, suffixes, dest_img_path, dest_label_path, split_name):
    """
    æ ¹æ®ç»™å®šçš„æ–‡ä»¶åˆ—è¡¨å’ŒåŽç¼€ï¼Œå¤åˆ¶å›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶ã€‚
    """
    count = 0
    for base_name in file_list:
        source_label_file = SOURCE_LABELS_DIR / f"{base_name}.txt"
        
        if not source_label_file.exists():
            continue

        for suffix in suffixes:
            source_image_file = SOURCE_IMAGES_DIR / f"{base_name}_{suffix}.png"
            if source_image_file.exists():
                # å¤åˆ¶å¹¶é‡å‘½åå›¾åƒå’Œæ ‡ç­¾æ–‡ä»¶ï¼Œä½¿å®ƒä»¬ä¸€ä¸€å¯¹åº”
                dest_image_file = dest_img_path / f"{base_name}_{suffix}.png"
                dest_label_file = dest_label_path / f"{base_name}_{suffix}.txt" # æ ‡ç­¾æ–‡ä»¶åä¸Žå›¾åƒååŒ¹é…
                
                shutil.copy(source_image_file, dest_image_file)
                shutil.copy(source_label_file, dest_label_file)
                count += 1
    print(f"  -> æˆåŠŸå¤åˆ¶äº† {count} å¼ å›¾åƒåŠå…¶æ ‡ç­¾åˆ° {split_name} é›†ã€‚")


def create_yaml_file(dataset_path, class_names):
    """
    ä¸ºæ•°æ®é›†åˆ›å»º .yaml é…ç½®æ–‡ä»¶ã€‚
    """
    yaml_content = f"""
# YOLOv8 Dataset Configuration File

path: {dataset_path.resolve()}  # æ•°æ®é›†æ ¹ç›®å½• (ç»å¯¹è·¯å¾„)
train: images/train  # è®­ç»ƒé›†å›¾ç‰‡ç›®å½• (ç›¸å¯¹äºŽ 'path')
val: images/val      # éªŒè¯é›†å›¾ç‰‡ç›®å½• (ç›¸å¯¹äºŽ 'path')

# Classes
names:
"""
    for i, name in enumerate(class_names):
        yaml_content += f"  {i}: {name}\n"

    with open(dataset_path / "dataset.yaml", "w", encoding='utf-8') as f:
        f.write(yaml_content)
    print(f"  -> å·²åœ¨ '{dataset_path}' ä¸­åˆ›å»º dataset.yamlã€‚")


if __name__ == "__main__":
    create_yolo_datasets()